{"posts":[{"title":"Biography","text":"Currently a second-year PhD student at The Hong Kong Polytechnic University under the supervision of Prof. Xiao-Ming Wu, and a visiting scholar at National University of Singapore under the supervision of Prof. Min-Yen Kan, Qijiong Liu (刘奇煚, Jyonn) obtained his B.Eng. and M.Eng. degrees from the School of Computer Science and Technology at Zhejiang University in 2018 and 2021, respectively. During his internships at Huawei, he worked on research projects related to recommender system and information retrieval. In 2020 and 2022, he worked as a research intern at Huawei Noah’s Ark Lab (Shenzhen) and Huawei Terminal Business Group (Nanjing), mentored by Jieming Zhu and Li Jiang, respectively. Moreover, he completed a one-year internship in San Jose during his senior year as part of the Cisco International Internship Program (CIIP), directed by Debo Dutta. To learn more about Qijiong Liu and access his resume, you can click on this link.","link":"/2020/10/01/Biography/"},{"title":"ONCE: Boosting Content-based Recommendation with Both Open- and Closed-source Large Language Models","text":"Our paper about combining large language models and recommender systems got updated in arXiv!First version (i.e., GENRE) discussed the use of closed-source LLMs (e.g., GPT-3.5) in recommender systems, while this version (i.e., ONCE) further combines open-source LLMs (e.g., LLaMA) and closed-source LLMs in recommender systems. Qijiong LIU, Nuo Chen, Tetsuya Sakai, and Xiao-Ming WU#[Code] [Paper] AbstractPersonalized content-based recommender systems have become indispensable tools for users to navigate through the vast amount of content available on platforms like daily news websites and book recommendation services. However, existing recommenders face significant challenges in understanding the content of items. Large language models (LLMs), which possess deep semantic comprehension and extensive knowledge from pretraining, have proven to be effective in various natural language processing tasks. In this study, we explore the potential of leveraging both open- and closed-source LLMs to enhance content-based recommendation. With open-source LLMs, we utilize their deep layers as content encoders, enriching the representation of content at the embedding level. For closed-source LLMs, we employ prompting techniques to enrich the training data at the token level. Through comprehensive experiments, we demonstrate the high effectiveness of both types of LLMs and show the synergistic relationship between them. Notably, we observed a significant relative improvement of up to 19.32% compared to existing state-of-the-art recommendation models. These findings highlight the immense potential of both open- and closed-source of LLMs in enhancing content-based recommendation systems. We will make our code and LLM-generated data available for other researchers to reproduce our results. Citation12345678@misc{liu2023once, title={ONCE: Boosting Content-based Recommendation with Both Open- and Closed-source Large Language Models}, author={Qijiong Liu and Nuo Chen and Tetsuya Sakai and Xiao-Ming Wu}, year={2023}, eprint={2305.06566}, archivePrefix={arXiv}, primaryClass={cs.IR}}","link":"/2023/08/31/Research-ONCE/"},{"title":"Only Encode Once: Making Content-based News Recommender Greener","text":"Our paper about green recommendation got published in arXiv! Qijiong LIU, Jieming Zhu, Quanyu Dai, and Xiao-Ming WU#[Code] [Paper] AbstractLarge pretrained language models (PLM) have become de facto news encoders in modern news recommender systems, due to their strong ability in comprehending textual content. These huge Transformer-based architectures, when finetuned on recommendation tasks, can greatly improve news recommendation performance. However, the PLM-based pretrain-finetune framework incurs high computational cost and energy consumption, primarily due to the extensive redundant processing of news encoding during each training epoch. In this paper, we propose the ``Only Encode Once’’ framework for news recommendation (OLEO), by decoupling news representation learning from downstream recommendation task learning. The decoupled design makes content-based news recommender as green and efficient as id-based ones, leading to great reduction in computational cost and training resources. Extensive experiments show that our OLEO framework can reduce carbon emissions by up to 13 times compared with the state-of-the-art pretrain-finetune framework and maintain a competitive or even superior performance level. The source code is released for reproducibility. Citation12345678@misc{liu2023only, title={Only Encode Once: Making Content-based News Recommender Greener}, author={Qijiong Liu and Jieming Zhu and Quanyu Dai and Xiao-Ming Wu}, year={2023}, eprint={2308.14155}, archivePrefix={arXiv}, primaryClass={cs.IR}}","link":"/2023/08/29/Research-OLEO/"},{"title":"Brand New Start at NUS","text":"I am very excited to announce that I will be joining the National University of Singapore as a visiting scholar in July 2023, working with Prof. Min-Yen Kan. The research topic will be related to information retrieval and multimodal learning. I am looking forward to this new journey!","link":"/2023/07/18/Brand-New-Start-at-NUS/"},{"title":"A First Look at LLM-powered Generative News Recommendation","text":"Our paper about combining large language models and recommender systems got published in arXiv! Qijiong LIU, Nuo Chen, Tetsuya Sakai, and Xiao-Ming WU#[Code] [Paper] AbstractPersonalized news recommendation systems have become essential tools for users to navigate the vast amount of online news content, yet existing news recommenders face significant challenges such as the cold-start problem, user profile modeling, and news content understanding. Previous works have typically followed an inflexible routine to address a particular challenge through model design, but are limited in their ability to understand news content and capture user interests. In this paper, we introduce GENRE, an LLM-powered generative news recommendation framework, which leverages pretrained semantic knowledge from large language models to enrich news data. Our aim is to provide a flexible and unified solution for news recommendation by moving from model design to prompt design. We showcase the use of GENRE for personalized news generation, user profiling, and news summarization. Extensive experiments with various popular recommendation models demonstrate the effectiveness of GENRE. We will publish our code and data for other researchers to reproduce our work. Citation12345678@misc{liu2023look, title={A First Look at LLM-Powered Generative News Recommendation}, author={Qijiong Liu and Nuo Chen and Tetsuya Sakai and Xiao-Ming Wu}, year={2023}, eprint={2305.06566}, archivePrefix={arXiv}, primaryClass={cs.IR}}","link":"/2023/05/12/Research-GENRE/"},{"title":"FANS: Fast Non-autoregressive Sequential Generation for Item List Continuation","text":"Our paper about list completion got accepted by TheWebConf 2023! Qijiong LIU, Jieming ZHU, Jiahao WU, Tiandeng WU, Zhenhua Dong, and Xiao-Ming WU#[Code] [Paper] AbstractUser-curated item lists, such as video-based playlists on Youtube and book-based lists on Goodreads, have become prevalent for sharing content on online platforms. Item list continuation is proposed to model the overall trend of lists and predict subsequent lists. Recently, Transformer-based models have shown promise in comprehending contextual information and capturing item relationships in a list. However, it is difficult to deploy them in real-time industrial scenarios, mainly due to the time-consuming nature of the autoregressive generation mechanism used in them. In this paper, we propose a novel fast non-autoregressive sequence generation model, namely FANS, to accelerate inference efficiency and improve inference quality for item list continuation. First, we use a non-autoregressive generation mechanism to decode next $K$ items simultaneously instead of one-by-one as in existing models. Then, we design a two-stage classifier to replace the vanilla classifier used in current Transformer-based models to further reduce the decoding time. Moreover, to improve inference quality of non-autoregressive generation, we employ a curriculum learning strategy to optimize training. Extensive experiments on four real-world item list continuation datasets including Zhihu, Spotify, AotM, and Goodreads demonstrate that our FANS model can significantly improve inference efficiency (up to 8.7x) while achieving competitive or better inference quality compared with state-of-the-art autoregressive models. We also validate the efficiency of FANS in an industrial system. The source code and data are provided for reproducing the reported results. Citation12345678@inproceedings{liu2023fans, title = {FANS: Fast Non-Autoregressive Sequence Generation for Item List Continuation}, author = {Liu, Qijiong and Zhu, Jieming and Wu, Jiahao and Wu, Tiandeng and Dong, Zhenhua and Wu, Xiao-Ming}, booktitle = {Proceedings of the ACM Web Conference 2023}, month = {may}, year = {2023}, address = {Austin, Texas, USA}}","link":"/2023/01/25/Research-FANS/"},{"title":"Continual Graph Convolutional Networks for Text Classification","text":"Our paper about list completion got accepted by AAAI 2023! Tiandeng WU*, Qijiong LIU*, Yao HUANG, Yi CAO, Xiao-Ming WU#, and Jiandong DING#*Equal contribution (co-first authors). Author ordering determined by dice rolling.[Code] [Paper] AbstractTo capture global non-consecutive and long-distance semantic information, graph convolutional network (GCN) has been widely used for text classification. While GCN-based methods have achieved great success in offline evaluations, they usually construct fixed document-token graphs and cannot perform inference on new documents. It is still a challenge to apply GCNs in online systems which need to infer continual text data. In this work, we present a Continual GCN model, short as ContGCN, to generalize inferences from observed documents to unobserved documents. Concretely, we propose a novel global-token-local-document paradigm to dynamically update the document-token graph in every batch for any online system during both training and testing phases. Moreover, we design an occurrence memory module and a self-supervised contrastive learning objective to update the proposed ContGCN in any online system in a label-free manner. Extensive offline experiments conducted on five public datasets demonstrate that our proposed ContGCN can significantly improve inference quality. A 3-month A/B test on our internal online system shows ContGCN achieves 8.86% performance gain compared with state-of-the-art methods. Citation12345678910111213@inproceedings{wu2023contgcn, title = &quot;Continual Graph Convolutional Networks for Text Classification&quot;, author = &quot;Wu, Tiandeng and Liu, Qijiong and Cao, Yi and Huang, Yao and Wu, Xiaoming and Ding, Jiandong&quot;, booktitle = &quot;Proceedings of the 37th AAAI Conference on Artificial Intelligence&quot;, month = feb, year = &quot;2023&quot;, address = &quot;Washington, DC, United States&quot;}","link":"/2022/11/19/Research-ContGCN/"},{"title":"SmartDict: Dynamic pointing of values in Python dictionaries","text":"IntroductionChain assignment of dictionary values, which can be used to reduce redundancy in configuration dictionaries. Installation1pip install smartdict UsageAssuming the following configuration dictionary exists: 1234567891011121314{ &quot;dataset&quot;: &quot;spotify&quot;, &quot;load&quot;: { &quot;train_path&quot;: &quot;~/data/spotify/train&quot;, &quot;dev_path&quot;: &quot;~/data/spotify/dev&quot;, &quot;test_path&quot;: &quot;~/data/spotify/test&quot; }, &quot;network&quot;: { &quot;num_hidden_layers&quot;: 3, &quot;num_attention_heads&quot;: 8, &quot;hidden_size&quot;: 64 }, &quot;store&quot;: &quot;checkpoints/spotify/3L8H/&quot;} It can be observed that many paths such as train_path are related to the dataset name dataset, and the store storage path is related to the dataset name, network structure, and so on. Additionally, if the dataset or network structure is changed, the configuration dictionary needs to be modified complexly. One solution is to handle the redundant information in Python code. For example, the value of train_path is directly assigned to train, in the code as follows: 1data['load']['train_path'] = os.path.join('~', 'data', data['dataset'], data['load']['train_path']) However, this method is not flexible enough. As the number of configuration properties increases, the amount of code required also increases linearly, making the preprocessing of configuration data slightly cumbersome. Normal String Referencing ${}We propose to construct the dictionary values as dynamic data, for example: 123456789101112131415{ &quot;dataset&quot;: &quot;spotify&quot;, &quot;load&quot;: { &quot;base_path&quot;: &quot;~/data/${dataset}&quot;, &quot;train_path&quot;: &quot;${load.base_path}/train&quot;, &quot;dev_path&quot;: &quot;${load.base_path}/dev&quot;, &quot;test_path&quot;: &quot;${load.base_path}/test&quot; }, &quot;network&quot;: { &quot;num_hidden_layers&quot;: 3, &quot;num_attention_heads&quot;: 8, &quot;hidden_size&quot;: 64 }, &quot;store&quot;: &quot;checkpoints/${dataset}/${network.num_hidden_layers}L${network.num_attention_heads}H/&quot;} The value of train_path is dynamically constructed by referencing the value of base_path and train, and the value of store is dynamically constructed by referencing the value of dataset, network.num_hidden_layers, and network.num_attention_heads. The above configuration dictionary can be processed as follows: 123456789101112# solution 1import smartdictdata = smartdict.parse(data)# solution 2from smartdict import DictCompilercompiler = DictCompiler(data)data = compiler.parse()print(data['load']['base_path']) # =&gt; ~/data/spotifyprint(data['load']['dev_path']) # =&gt; ~/data/spotify/devprint(data['store']) # =&gt; checkpoints/spotify/3L8H/ Highly recommended to use smartdict along with Oba: 123456from oba import Objdata = Obj(data)print(data.load.base_path) # =&gt; ~/data/spotifyprint(data.load.dev_path) # =&gt; ~/data/spotify/devprint(data.store) # =&gt; checkpoints/spotify/3L8H/ Full Match Referencing ${}$In the Normal String Referencing ${} solution, the value of the configuration dictionary is a string. The reference is performed through a chain path (separated by .) within the ${} identifier. The Full Match Reference only allows the property value to be completely covered by the identifier ${}$. For example: 123456789101112131415161718import obaimport smartdictdata = dict( a='${b.v.1}+1', # Normal String Referencing b='${c}$', # Full Match Referencing c=dict( l=23, v=('are', 'you', 'ok'), ))data = smartdict.parse(data)print(data['b']) # =&gt; {'l': 23, 'v': ('are', 'you', 'ok')}data = oba.Obj(data)print(data.a) # =&gt; you+1print(data.b.l) # =&gt; 23 In which b is identical to c through the Full Match Reference. Summon MagicSometimes, we may wish to generate the path through timestamps or random numbers. We can first construct the following two classes: TimestampMagic 12345678910111213141516import datetimeclass TimestampMagic(dict): def __init__(self): dict.__init__(self, {}) def __contains__(self, item): return True def __getitem__(self, item): now = datetime.datetime.now() if item == 'str': return now.strftime('%y%m%d-%H%M%S') else: return hex(int(now.timestamp()))[2:] RandomMagic 123456789101112131415import randomimport stringclass RandomMagic(dict): chars = string.ascii_letters + string.digits def __init__(self): dict.__init__(self, {}) def __contains__(self, item): return True def __getitem__(self, item): return ''.join([random.choice(self.chars) for _ in range(int(item))]) Then, we can use the following configuration dictionary: 12345678910111213141516import smartdictdata = dict( filename='${utils.time.str}/${utils.rand.4}.log', # Summon Magic, supported by smartdict&gt;=0.0.4)data.update(dict( utils=dict( rand=Rand(), time=Timing(), )))data = smartdict.parse(data)print(data['filename']) # =&gt; 20221110-123504/to1E.log The principle behind this is to override the class’s [] operator to make it behave the same as a dictionary or list. LicenseMIT Links GitHub PyPI Documentation Chinese version","link":"/2022/11/09/SmartDict-Dynamic-pointing-of-values-in-Python-dictionaries/"},{"title":"Oba: Converting a Python iterable object to an attribute class","text":"IntroductionConverting iterable objects (such as dictionaries, tuples, lists, sets, and subclasses) to access values using class attributes instead of square brackets []. Supports recursive operations and supports both getting and setting values. Installation1pip install oba Usage12345678910111213141516171819from oba import Oba# Convert a dictionary to an attribute classd = dict(a=[1, 2, 3], b=[4, dict(x=1)], c=dict(l='hello'))o = Obj(d)# Get valuesprint(o.a[2]) # =&gt; 3print(o.c.l) # =&gt; hello# Set valueso.b[1].x = 4print(Obj.raw(o.b[1])) # =&gt; {'x': 4}points = [dict(x=1, y=2), dict(x=-1, y=0), dict(x=0, y=1)]o = Obj(points)o[0].x += 1print(Obj.raw(o[0])) # =&gt; {'x': 2, 'y': 2} Comparisonnamedtuple (built-in module)12345678from collections import namedtupleo = namedtuple('Struct', d.keys())(*d.values())print(o) # =&gt; Struct(a=[1, 2, 3], b=[4, {'x': 1}], c={'l': 'hello'})print(o.a) # =&gt; [1, 2, 3]o.x = 2 # =&gt; AttributeError bunch (package)1234567891011from bunch import Buncho = Bunch(d)print(o.a) # =&gt; [1, 2, 3]print(o.b['x']) # =&gt; 1print(o.b.x) # =&gt; KeyErroro.x = 2 # OK, editableo = Bunch(points) # =&gt; AttributeError json (built-in module)1234567891011121314151617import jsonclass Obj(object): def __init__(self, d): self.__dict__.update(d)o = json.loads(json.dumps(d), object_hook=Obj)print(o.a[2]) # =&gt; 3print(o.c.l) # =&gt; helloo.x = 2 # OK, editableo = Obj(points)o[0].x += 1print(o[0].x) # =&gt; 2 mock (package)1234567891011from mock import Mocko = Mock(**d)print(o.a) # =&gt; [1, 2, 3]print(o.b['x']) # =&gt; 1print(o.b.x) # =&gt; KeyErroro.x = 2 # OKo = Mock(*points) # =&gt; TypeError Summary namedtuple bunch (2011) json mock (2020) oba (2022) built-in ✓ ✗ 11K ✓ ✗ 28K ✗ 2K recursive ✗ ✗ ✓ ✗ ✓ revert to raw ✗ ✓ (with extra operations) ✗ ✗ ✓ editable ✗ ✓ ✓ ✓ ✓ iterable ✓ (no keys) ✓ ✗ ✗ ✓ support dict ✓ ✓ ✓ ✓ ✓ support list ✓ ✗ ✓ ✗ ✓ support tuple ✗ ✗ ✓ ✗ ✓ tolerable ✗ ✗ ✗ ✗ ✓ FeaturesAdditionally, Oba also has a unique tolerance for unknown attributes. In cases where some attributes do not exist, for example: 12d = dict(a=1)print(d['b']) # KeyError Other libraries will immediately raise an error. However, in some scenarios (such as reading configuration files), the absence of sub-attributes is a common problem, and we hope to be able to tolerate and monitor the existence of such errors. 123456789from oba import Objd = dict(a=1)o = Obj(d)print('x' in o) # =&gt; Falseif not o.x.y.z: # OK print('not exist') # =&gt; not existprint(o.x.y.z) # =&gt; ValueError: NoneObj (x.y.z) # locating the non-existent attribute chain Its internal implementation is that when an attribute does not exist, the object automatically switches to the NoneObj class and records the attribute chain. LicenseMIT Links GitHub PyPI Documentation Chinese version","link":"/2022/11/07/Oba-Converting-a-Python-iterable-object-to-an-attribute-class/"},{"title":"PREC: Boosting Deep CTR Prediction with a Plug-and-Play Pre-trainer for News Recommendation","text":"Our paper about list completion got accepted by COLING 2022! Qijiong LIU, Jieming ZHU, Quanyu Dai, Xiao-Ming WU#[Code] [Paper] AbstractUnderstanding news content is critical to improving the quality of news recommendation. To achieve this goal, recent studies have attempted to apply pre-trained language models (PLMs) such as BERT for semantic-enhanced news recommendation. Despite their great success in offline evaluation, it is still a challenge to apply such large PLMs in real-time ranking model due to the stringent requirement in inference and updating time. To bridge this gap, we propose a plug-and-play pre-trainer, namely PREC, to learn both user and news encoders through multi-task pre-training. Instead of directly leveraging sophisticated PLMs for end-to-end inference, we focus on how to use the derived user and item representations to boost the performance of conventional lightweight models for click-through-rate prediction. This enables efficient online inference as well as compatibility to conventional models, which would significantly ease the practical deployment. We validate the effectiveness of PREC through both offline evaluation on public datasets and online A/B testing in an industrial application. Citation1234567891011121314@inproceedings{liu2022prec, title = &quot;Boosting Deep {CTR} Prediction with a Plug-and-Play Pre-trainer for News Recommendation&quot;, author = &quot;Liu, Qijiong and Zhu, Jieming and Dai, Quanyu and Wu, Xiaoming&quot;, booktitle = &quot;Proceedings of the 29th International Conference on Computational Linguistics&quot;, month = oct, year = &quot;2022&quot;, address = &quot;Gyeongju, Republic of Korea&quot;, publisher = &quot;International Committee on Computational Linguistics&quot;, url = &quot;https://aclanthology.org/2022.coling-1.249&quot;, pages = &quot;2823--2833&quot;}","link":"/2022/08/17/Research-PREC/"}],"tags":[{"name":"python","slug":"python","link":"/tags/python/"},{"name":"package","slug":"package","link":"/tags/package/"},{"name":"research","slug":"research","link":"/tags/research/"},{"name":"life","slug":"life","link":"/tags/life/"},{"name":"recommender system","slug":"recommender-system","link":"/tags/recommender-system/"},{"name":"green AI","slug":"green-AI","link":"/tags/green-AI/"},{"name":"large language model","slug":"large-language-model","link":"/tags/large-language-model/"},{"name":"text classification","slug":"text-classification","link":"/tags/text-classification/"},{"name":"natural language processing","slug":"natural-language-processing","link":"/tags/natural-language-processing/"},{"name":"non-autoregressive generation","slug":"non-autoregressive-generation","link":"/tags/non-autoregressive-generation/"},{"name":"list completion","slug":"list-completion","link":"/tags/list-completion/"}],"categories":[{"name":"Development","slug":"Development","link":"/categories/Development/"},{"name":"Research","slug":"Research","link":"/categories/Research/"}],"pages":[{"title":"Qijiong LIU","text":"刘奇煚 A brief resume is available in PDF format (Jun. 2023 version). Contact Email: i@6-79.cn or jyonn.liu@connect.polyu.hk Phone: +852-6082-5304 Github: Jyonn Homepage: https://liu.qijiong.work Education2023-2024 (expected)Visiting Scholar, Computing; National University of Singapore (Singapore)2021-2024 (expected)PhD, Computing; The Hong Kong Polytechnic University (Hong Kong SAR)2018-2021MSc, Computer Science; Zhejiang University (China)2014-2018BSc, Software Engineering; Zhejiang University (China)Working ExperienceApr. 2022 - Aug. 2022Algorithm Intern; Huawei Business Group (Nanjing) Designed an item playlist continuation method, namely FANS, which improves the effectiveness of business music recommender system. Proposed a continual text classification method, namely ContGCN, which has been applied to the public opinion monitoring system. Sept. 2020 - Jan. 2021Algorithm Intern; Huawei Noah’s Ark Lab (Shenzhen) Designed a news recommendation pretraining method, namely RecBERT, which improves the effectiveness of business news recommender system. Aug. 2017 - Jul. 2018Algorithm Intern; Cisco Zeus Group (San Jose) Designed an AutoML system, namely CiscoAdvisor, supporting random forest and Bayesian parameter tuning. Contributed to the Kubeflow open source project in terms of hard disk scalability. PublicationsRecommender System (arXiv May 2023) A First Look at LLM-powered Generative News RecommendationQijiong LIU, Nuo Chen, Tetsuya Sakai, and Xiao-Ming WU#[Code] [Paper] (WWW2023) FANS: Fast Non-Autoregressive Sequence Generation for Item List ContinuationQijiong LIU, Jieming ZHU, Jiahao WU, Tiandeng WU, Zhenhua Dong, and Xiao-Ming WU#[Code] [Paper] (COLING2022) Boosting Deep CTR Prediction with a Plug-and-Play Pre-trainer for News RecommendationQijiong LIU, Jieming ZHU, Quanyu Dai, Xiao-Ming WU#[Code] [Paper] Natural Language Processing (AAAI2023) Continual Graph Convolutional Network for Text ClassificationQijiong LIU*, Tiandeng WU*, Yao HUANG, Yi CAO, Xiao-Ming WU#, and Jiandong DING#*Equal contribution (co-first authors). Author ordering determined by dice rolling.[Code] [Paper] (IJCAI2019) Weak Supervision Enhanced Generative Network for Question GenerationYutong WANG, Jiyuan ZHENG, Qijiong LIU, Zhou ZHAO#, Jun XIAO, Yueting ZHUANG[Paper] Award 2023. ACM Member 2023. AAAI Student Scholarship 2021. PolyU Research Postgraduate Scholarship 2021. Outstanding Graduate of Zhejiang University 2018. Outstanding Graduate of Zhejiang University 2016. Student of He Zhijun Class, Zhejiang University Teaching Experience 2023. Teaching Assistant, The Hong Kong Polytechnic University Course: COMP4121: E-Commerce Technology and Application Instructor: Prof. Chun Bun Henry CHAN 2022. Teaching Assistant, The Hong Kong Polytechnic University Course: COMP5523: Computer Vision and Image Processing Instructor: Prof. Xiao-Ming WU 2022. Teaching Assistant, The Hong Kong Polytechnic University Course: COMP5434: Big Data Computing Instructor: Prof. Jieming SHI 2021. Teaching Assistant, The Hong Kong Polytechnic University Course: COMP2011: Data Structures and Algorithms Instructor: Prof. Yixin CAO 2017. Teaching Assistant, Zhejiang University Course: Secure Programming Instructor: Prof. Tianlei HU Selected Side ProjectsMasterWhole | 2020 WeChat Official Account as a toolbox terminal, integrated with Toolbox project and theVideoOfChina project. Framework: Django Fancy tools: Exchange Rate Monitor. Monitor the exchange rate changes (based on data from BANK OF CHINA) and send the notification. Video Downloader. Download the video from popular websites, such as Xin Pianchang, Douyin. Open Source: [Code] QiqiX (奇奇十号 Qíqí Shíhào in Chinese) | 2020 Article comment mini program for WeChat Official Account, imitating the native WeChat interface. Framework: Wechat Mini Program (WXML, WXSS, JavaScript) Open Source: [Code], [Documentation] (Chinese) RusticCasket (浑天匣 Húntiān Xiá in Chinese) | 2017 Personal webdisk. The bachelor graduation project version uses ISTIO as the microservice architecture design. Framework: Django, Angular, ISTIO Unique Features: Resource Sharing. Users can share resources (files or folders) by generating a link. Resource Description. Users can add markdown-style descriptions to resources. Resource Cover. Users can add covers to resources based on a resource inheritance system. Open Source: [Backend], [Frontend], and [Website] (Chinese) More Projects... Open Source PackagesKubeflow (as a contributor) | 2018 Kubeflow is a machine learning toolkit for Kubernetes. It provides a set of tools to help data scientists and developers run machine learning workflows on Kubernetes.Pull Requests: tf-serving support model on NFS, website UniTok | 2021 A unified tokenizer tool for deep learning dataset tokenization. It supports tokenization of textual content, entity content, and customized format such as datetime. Open Source: [Code] and [Documentation] SmartDjango | 2019 A high-level packed development framework based on Django, supported by smartify and Oba projects. It is currently used in all my django-based projects. Features: Field Validation. Support for automatic field validation, such as maximum and minimum length in models.CharField and other custom rules. Powerful Request Validation. Support for complex request parameter validation and processing. Smart Model. Support for model-to-dict conversion and list pagination. Default Json Response. Adding one line in settings.py can automatically convert the response to json format. Open Source: [Code] and [Documentation] lEmoji | 2020 A versioned emoji character detection tool based on Unicode Emoji Standard and can be manually updated. For now (January 2023), the official package supports emoji detection from v1.0 to v15.0. Usage:1234from lEmoji import EMOJI_LIST_1_0, EMOJI_LIST # EMOJI_LIST is the latest version print('👨‍👩‍👧‍👦' in EMOJI_LIST_1_0) # Falseprint('👨‍👩‍👧‍👦' in EMOJI_LIST) # True Open Source: [Code] and [Documentation] smartdict | 2022 A smart dictionary tool that supports the reference of values in the dictionary by key, index, and slice. Usage:1234567import smartdictdata = dict(a=['hello', 'world'], b=&quot;${a}$&quot;, c=&quot;${b.0} python&quot;)data = smartdict.parse(data)print(data['b']) # ['hello', 'world']print(data['c']) # hello python Open Source: [Code] and [Documentation] AcknowledgementsAcademic Advisers Ms. Xiao-Ming WU (吴晓明) Mr. Jieming ZHU (朱杰明) Mr. Zhou ZHAO (赵洲) Family Mr. Ziyang LIU and Ms. Huiqin XU Ms. Yanping CHEN Mr. Jiyi CHEN and Ms. Dongmei TAO Mr. Xinqi XU, Mr. Xiangshui LIU, Ms. Jixian SHAO, and Ms. Zhuqiu SHAO Academic Tools PyTorch HuggingFace with transformers Developing Tools JetBrains with PyCharm and WebStorm GitHub with GitHub Student Developer Pack Python with PyPI Hexo","link":"/resume/index.html"},{"title":"projects","text":"MasterWhole | 2020WeChat Official Account as a toolbox terminal, integrated with Toolbox project and theVideoOfChina project. Framework: Django Fancy tools: Exchange Rate Monitor. Monitor the exchange rate changes (based on data from BANK OF CHINA) and send the notification. Video Downloader. Download the video from popular websites, such as Xin Pianchang, Douyin. Open Source: Code QiqiX (奇奇十号 Qíqí Shíhào in Chinese) | 2020Article comment mini program for WeChat Official Account, imitating the native WeChat interface. Framework: Wechat Mini Program (WXML, WXSS, JavaScript) Open Source: Code, Instruction (Chinese) RusticCasket (浑天匣 Húntiān Xiá in Chinese) | 2017Personal webdisk. The bachelor graduation project version uses ISTIO as the microservice architecture design. Framework: Django, Angular, ISTIO Unique Features: Resource Sharing. Users can share resources (files or folders) by generating a link. Resource Description. Users can add markdown-style descriptions to resources. Resource Cover. Users can add covers to resources based on a resource inheritance system. Open Source: Backend and Frontend Website: RusticCasket (Chinese) CosmosBook (齐天簿 Qítian Bù in Chinese) | 2017SSO(Single-Sign-On) central authorization platform based on OAuth2.0. Framework: Django, Angular Unique Features: Central Authorization. Users can use their account to log in to other applications. Application Management. Developers can manage their own applications and set the permissions of the application. Application Center. Variety of applications such as RusticCasket, Shelter, WorldOutlook can be accessed through the application center. Open Source: Backend and Frontend Website: CosmosBook (Chinese) Shelter (投明 Tóumíng in Chinese) | 2019Online replica of iOS Notes app. Framework: Django Open Source: Code Website: Shelter (Chinese) WorldOutlook (世界观 Shìjiè Guān in Chinese) | 2018Multi-user Shad0wS0cks server management platform. Before October 2021, the server built a channel from China to the outside world, and after October 2021, the channel is built from the outside world to China. Framework: Django Open Source: Backend and Frontend Website: WorldOutlook (Chinese)","link":"/projects/index.html"},{"title":"","text":"div.avatar { width:200px; height:200px; background: url('/images/avatar.jpg') 50% 50% no-repeat; background-size: cover; border-radius: 100px; margin: 0 auto 20px auto; border: 3px solid #eee; transition: 0.5s; cursor: pointer; } div.avatar:hover { border: 5px solid #2bbc8a; }","link":"/styles/avatar.css"},{"title":"","text":"img.middle { display: block; margin: 0 auto; width:200px; }","link":"/styles/resume.css"}]}